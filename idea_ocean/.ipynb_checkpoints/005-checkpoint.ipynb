{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings  \n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 目錄存在，不建立。\n",
      "vis  目錄存在，不建立。\n",
      "sub  目錄存在，不建立。\n"
     ]
    }
   ],
   "source": [
    "lis = ['data', 'vis', 'sub']\n",
    "for i in lis:\n",
    "    if os.path.isdir(i):\n",
    "        print(\"%-5s目錄存在，不建立。\"%i)\n",
    "    else:\n",
    "        os.makedirs(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './data/train.csv'\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "test_filename = './data/test.csv'\n",
    "test = pd.read_csv(test_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "- y = LEVEL\n",
    "- 文字: Station, County, Location\n",
    "- 有nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Station', 'Season', 'County', 'Location', 'Lat', 'Lon', '縣市', '海岸段',\n",
       "       'Region', 'Seat', 'Shore shape', 'Substrate type', '1暴露岩岸', '2暴露人造結構物',\n",
       "       '3暴露岩盤', '4沙灘', '5砂礫混合灘', '6礫石灘', '7開闊潮間帶', '8遮蔽岩岸', '9遮蔽潮間帶', '10遮蔽濕地',\n",
       "       'Plastic bottle container', 'Disposable cup / straw / tableware',\n",
       "       'Plastic bag', 'Foam material', 'Float', 'Fishing nets and ropes',\n",
       "       'Fishing equipment', 'Cigarette and lighter', 'Glass jar', 'Metal',\n",
       "       'Paper', 'Others', 'LEVEL'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['1暴露岩岸', '2暴露人造結構物', '3暴露岩盤', '4沙灘', '5砂礫混合灘', '6礫石灘', '7開闊潮間帶',\n",
       "       '8遮蔽岩岸', '9遮蔽潮間帶', '10遮蔽濕地'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[df.isna().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "print(len(df[df.isna().T.any()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1暴露岩岸</th>\n",
       "      <th>2暴露人造結構物</th>\n",
       "      <th>3暴露岩盤</th>\n",
       "      <th>4沙灘</th>\n",
       "      <th>5砂礫混合灘</th>\n",
       "      <th>6礫石灘</th>\n",
       "      <th>7開闊潮間帶</th>\n",
       "      <th>8遮蔽岩岸</th>\n",
       "      <th>9遮蔽潮間帶</th>\n",
       "      <th>10遮蔽濕地</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1暴露岩岸  2暴露人造結構物  3暴露岩盤  4沙灘  5砂礫混合灘  6礫石灘  7開闊潮間帶  8遮蔽岩岸  9遮蔽潮間帶  10遮蔽濕地\n",
       "3      NaN       NaN    NaN  NaN     NaN   NaN     NaN    NaN     NaN     NaN\n",
       "7      NaN       NaN    NaN  NaN     NaN   NaN     NaN    NaN     NaN     NaN\n",
       "11     NaN       NaN    NaN  NaN     NaN   NaN     NaN    NaN     NaN     NaN\n",
       "15     NaN       NaN    NaN  NaN     NaN   NaN     NaN    NaN     NaN     NaN\n",
       "19     NaN       NaN    NaN  NaN     NaN   NaN     NaN    NaN     NaN     NaN\n",
       "..     ...       ...    ...  ...     ...   ...     ...    ...     ...     ...\n",
       "302    NaN       NaN    NaN  NaN     NaN   NaN     NaN    NaN     NaN     NaN\n",
       "306    NaN       NaN    NaN  NaN     NaN   NaN     NaN    NaN     NaN     NaN\n",
       "310    NaN       NaN    NaN  NaN     NaN   NaN     NaN    NaN     NaN     NaN\n",
       "314    NaN       NaN    NaN  NaN     NaN   NaN     NaN    NaN     NaN     NaN\n",
       "318    NaN       NaN    NaN  NaN     NaN   NaN     NaN    NaN     NaN     NaN\n",
       "\n",
       "[76 rows x 10 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = df[df.isna().T.any()]\n",
    "a.loc[:,a.isna().any()]\n",
    "\n",
    "# 所有包含缺失值的資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['Season'].unique()\n",
    "\n",
    "# 發現都出現在Season = 4的時候 可能冬天沒去看?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1暴露岩岸</th>\n",
       "      <th>2暴露人造結構物</th>\n",
       "      <th>3暴露岩盤</th>\n",
       "      <th>4沙灘</th>\n",
       "      <th>5砂礫混合灘</th>\n",
       "      <th>6礫石灘</th>\n",
       "      <th>7開闊潮間帶</th>\n",
       "      <th>8遮蔽岩岸</th>\n",
       "      <th>9遮蔽潮間帶</th>\n",
       "      <th>10遮蔽濕地</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>319 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1暴露岩岸  2暴露人造結構物  3暴露岩盤  4沙灘  5砂礫混合灘  6礫石灘  7開闊潮間帶  8遮蔽岩岸  9遮蔽潮間帶  10遮蔽濕地\n",
       "0      0.0       0.0    1.0  0.0     0.0   0.0     1.0    0.0     0.0     0.0\n",
       "1      0.0       0.0    1.0  0.0     0.0   0.0     0.0    0.0     0.0     0.0\n",
       "2      0.0       0.0    1.0  0.0     0.0   0.0     0.0    0.0     0.0     0.0\n",
       "3      NaN       NaN    NaN  NaN     NaN   NaN     NaN    NaN     NaN     NaN\n",
       "4      0.0       0.0    0.0  1.0     0.0   0.0     0.0    0.0     0.0     0.0\n",
       "..     ...       ...    ...  ...     ...   ...     ...    ...     ...     ...\n",
       "314    NaN       NaN    NaN  NaN     NaN   NaN     NaN    NaN     NaN     NaN\n",
       "315    0.0       0.0    0.0  1.0     0.0   0.0     0.0    0.0     0.0     0.0\n",
       "316    0.0       1.0    0.0  1.0     0.0   0.0     0.0    0.0     0.0     0.0\n",
       "317    0.0       0.0    0.0  0.0     0.0   1.0     0.0    0.0     0.0     0.0\n",
       "318    NaN       NaN    NaN  NaN     NaN   NaN     NaN    NaN     NaN     NaN\n",
       "\n",
       "[319 rows x 10 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:,df.isna().any()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 把Season 4的部分用眾數補起來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E02' 'E03' 'E05' 'E06' 'E08' 'E09' 'HL02' 'HL03' 'HL05' 'HL06' 'HL08'\n",
      " 'HL09' 'HL11' 'HL12' 'M01' 'M02' 'M04' 'M05' 'M07' 'M08' 'M10' 'M11'\n",
      " 'M13' 'M14' 'M16' 'M17' 'M19' 'M20' 'M22' 'M23' 'N01' 'N02' 'N04' 'N05'\n",
      " 'N07' 'N08' 'N10' 'N11' 'N13' 'N14' 'N16' 'N17' 'N19' 'N20' 'N22' 'N23'\n",
      " 'SE03' 'SE04' 'SE06' 'SE07' 'SE09' 'SE10' 'SE12' 'SE13' 'SE15' 'SE16'\n",
      " 'SE18' 'SE19' 'SE21' 'SE22' 'SE24' 'SW01' 'SW03' 'SW04' 'SW06' 'SW07'\n",
      " 'SW09' 'SW10' 'SW12' 'SW13' 'SW15' 'SW16' 'SW18' 'SW19' 'SW21' 'SW22'\n",
      " 'SW24' 'SW25' 'TT02' 'TT03']\n"
     ]
    }
   ],
   "source": [
    "df_copy = df.copy()\n",
    "nan_col = df_copy.columns[df_copy.isna().any()]\n",
    "\n",
    "lis = df_copy['Station'].unique()\n",
    "print(lis)\n",
    "for i in lis:\n",
    "    cond = df_copy['Station'] == i\n",
    "    \n",
    "    df_copy.loc[cond, nan_col] = df_copy.loc[cond, nan_col].fillna(0, axis = 0)\n",
    "    df_copy.loc[cond, nan_col] = df_copy.loc[cond, nan_col].mode().values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              大溪\n",
       "1              大溪\n",
       "2              大溪\n",
       "3              大溪\n",
       "4              頭城\n",
       "          ...    \n",
       "314            長光\n",
       "315           白桑安\n",
       "316    白桑安/長濱觀景平台\n",
       "317    白桑安/長濱觀景平台\n",
       "318            寜埔\n",
       "Name: Location, Length: 319, dtype: object"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy.pop('County')\n",
    "df_copy.pop('Station')\n",
    "df_copy.pop('Location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_copy.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 有文字的用label enc 換掉\n",
    "Station, County, Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import preprocessing\n",
    "# le = preprocessing.LabelEncoder()\n",
    "\n",
    "# # lis = ['Station', 'County', 'Location']\n",
    "# lis = list(df.dtypes[df.dtypes == 'object'].index)\n",
    "\n",
    "# labelEnc_dict = dict() \n",
    "# for i in lis:\n",
    "#     labelEnc_dict[i] = preprocessing.LabelEncoder()\n",
    "#     labelEnc_dict[i].fit(df_copy[i])\n",
    "#     df_copy.loc[:, i] = labelEnc_dict[i].transform(df_copy[i])\n",
    "# df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "def xgb_model(epochs, objective, booster, max_depth):\n",
    "    model = XGBRegressor(n_estimators = epochs,\n",
    "                         max_depth = max_depth,\n",
    "                         objective = objective,\n",
    "                         tree_method = 'gpu_hist',\n",
    "                         booster = booster,\n",
    "                         verbosity = 1)\n",
    "    return model\n",
    "\n",
    "def lr_curve(results, ylabel, title):\n",
    "    keys = list(results.keys())\n",
    "    item = list(results[keys[0]].keys())[0]\n",
    "    epochs = len(results[keys[0]][item])\n",
    "    x_axis = range(0, epochs)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.plot(x_axis, results[\"validation_0\"][item], label=\"Train\")\n",
    "    ax.plot(x_axis, results[\"validation_1\"][item], label=\"Test\")\n",
    "    ax.legend()\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# def eva_metric(y_true, y_pred):\n",
    "#     dic = {\n",
    "#         'f1_score':f1_score,\n",
    "#        'confusion_matrix': confusion_matrix,\n",
    "#        'multilabel_confusion_matrix': multilabel_confusion_matrix}\n",
    "    \n",
    "#     r2 = []\n",
    "#     eval_res = {}\n",
    "#     for i in dic.keys():\n",
    "#         if i == 'f1_score':\n",
    "# #             r2.append(score)\n",
    "#             score = dic[i](y_true, y_pred, average = 'macro')\n",
    "#         else:\n",
    "#             score = dic[i](y_true, y_pred)\n",
    "# #         print(i, score)\n",
    "#         eval_res[i] = score\n",
    "\n",
    "#     return eval_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, average_precision_score, multilabel_confusion_matrix, confusion_matrix\n",
    "\n",
    "def confusion_matrix_eval(y_true, y_pred, classes):\n",
    "    eva_result = dict()\n",
    "    eva_result['confusion_matrix'] = confusion_matrix(y_true, y_pred)\n",
    "    eva_result['multilabel_confusion_matrix'] = multilabel_confusion_matrix(y_true, y_pred, labels=range(classes))\n",
    "    \n",
    "    sns.heatmap(eva_result['confusion_matrix'], annot=True)\n",
    "    plt.title('樣本總數: '+str(np.sum(eva_result['confusion_matrix'])))\n",
    "    plt.show()\n",
    "\n",
    "    # 各個類別的混淆矩陣\n",
    "    plt.figure(12, figsize=(28,15))\n",
    "    num = len(eva_result['multilabel_confusion_matrix'])\n",
    "#     print(eva_result['multilabel_confusion_matrix'])\n",
    "    positive = np.unique(y_true, return_counts=True)[1]\n",
    "    for tmp in range(1, num+1):\n",
    "        plt.subplot(3, (num//2), tmp)\n",
    "        a = np.array(eva_result['multilabel_confusion_matrix'][tmp-1])\n",
    "\n",
    "        sns.heatmap(a, annot=True)\n",
    "        plt.title('樣本總數: '+str(np.sum(a))+' 正樣本占比: '+'%d / %d' % (positive[tmp-1], np.sum(a)))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eva_metric(y_true, y_pred):\n",
    "    dic = {'mse':mean_squared_error,\n",
    "       'mae': mean_absolute_error,\n",
    "       'r2_score': r2_score}\n",
    "    \n",
    "    r2 = []\n",
    "    eval_res = {}\n",
    "    for i in dic.keys():\n",
    "        score = dic[i](y_true, y_pred)\n",
    "#         print(i, score)\n",
    "        eval_res[i] = score\n",
    "        if i == 'r2_score':\n",
    "            r2.append(score)\n",
    "    return eval_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_copy.copy()\n",
    "y = X.pop('LEVEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto-sklearn results:\n",
      "  Dataset name: ocean_001\n",
      "  Metric: accuracy\n",
      "  Best validation score: 0.313901\n",
      "  Number of target algorithm runs: 246\n",
      "  Number of successful target algorithm runs: 242\n",
      "  Number of crashed target algorithm runs: 2\n",
      "  Number of target algorithms that exceeded the time limit: 2\n",
      "  Number of target algorithms that exceeded the memory limit: 0\n",
      "\n",
      "Accuracy score 0.3125\n"
     ]
    }
   ],
   "source": [
    "import autosklearn.classification\n",
    "import sklearn.model_selection\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "import pickle\n",
    "\n",
    "# X, y = sklearn.datasets.load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    sklearn.model_selection.train_test_split(X, y, test_size = 0.3, random_state=12)\n",
    "\n",
    "automl = autosklearn.classification.AutoSklearnClassifier(\n",
    "    time_left_for_this_task=600,\n",
    "    per_run_time_limit=80,\n",
    "#     tmp_folder='./tmp/autosklearn_parallel_1_example_tmp',\n",
    "#     output_folder='./tmp/autosklearn_parallel_1_example_out',\n",
    "#     include_estimators = ['random_forest', 'libsvm_svc', ],\n",
    "#     resampling_strategy='holdout',\n",
    "#     resampling_strategy_arguments={'train_size': 0.8},\n",
    "    resampling_strategy='cv',\n",
    "    resampling_strategy_arguments={'folds': 5},\n",
    "    ensemble_nbest = 30, \n",
    "    \n",
    "    \n",
    "    n_jobs=8,\n",
    "    # Each one of the 4 jobs is allocated 3GB\n",
    "    memory_limit=10240,\n",
    "    seed=5,\n",
    ")\n",
    "automl.fit(X_train, y_train, dataset_name='ocean_001')\n",
    "\n",
    "# Print statistics about the auto-sklearn run such as number of\n",
    "# iterations, number of models failed with a time out.\n",
    "print(automl.sprint_statistics())\n",
    "y_hat = automl.predict(X_test)\n",
    "print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_hat))\n",
    "\n",
    "\n",
    "\n",
    "# save model\n",
    "with open('./tmp/iris-classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(automl, f)\n",
    "\n",
    "# load model\n",
    "with open('./tmp/iris-classifier.pkl', 'rb') as f:\n",
    "    loaded_classifier = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LEVEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E01_1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E01_2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E01_3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E01_4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E04_1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  LEVEL\n",
       "0  E01_1    NaN\n",
       "1  E01_2    NaN\n",
       "2  E01_3    NaN\n",
       "3  E01_4    NaN\n",
       "4  E04_1    NaN"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_filename = './data/test.csv'\n",
    "df = pd.read_csv(test_filename)\n",
    "\n",
    "tmp = pd.read_csv('data/submission.csv')\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 把地理因素補起來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E01' 'E04' 'E07' 'HL01' 'HL04' 'HL07' 'HL10' 'LI10' 'M03' 'M06' 'M09'\n",
      " 'M12' 'M15' 'M18' 'M21' 'M24' 'N03' 'N06' 'N09' 'N12' 'N15' 'N18' 'N21'\n",
      " 'N24' 'SE05' 'SE08' 'SE11' 'SE14' 'SE17' 'SE20' 'SE23' 'SW02' 'SW05'\n",
      " 'SW08' 'SW11' 'SW14' 'SW17' 'SW20' 'SW23' 'TT01' 'TT04']\n"
     ]
    }
   ],
   "source": [
    "df_copy = df.copy()\n",
    "nan_col = df_copy.columns[df_copy.isna().any()]\n",
    "\n",
    "lis = df_copy['Station'].unique()\n",
    "print(lis)\n",
    "for i in lis:\n",
    "    cond = df_copy['Station'] == i\n",
    "    \n",
    "    df_copy.loc[cond, nan_col] = df_copy.loc[cond, nan_col].fillna(0, axis = 0)\n",
    "    df_copy.loc[cond, nan_col] = df_copy.loc[cond, nan_col].mode().values[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 做submmision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       石城\n",
       "1       石城\n",
       "2       石城\n",
       "3       石城\n",
       "4       永鎮\n",
       "      ... \n",
       "158    八仙北\n",
       "159    石雨傘\n",
       "160    石雨傘\n",
       "161    石雨傘\n",
       "162    石雨傘\n",
       "Name: Location, Length: 163, dtype: object"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy.pop('County')\n",
    "df_copy.pop('Station')\n",
    "df_copy.pop('Location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('./tmp/iris-classifier.pkl', 'rb') as f:\n",
    "    loaded_classifier = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-06-05 15:21:20'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "now_time = datetime.datetime.now()\n",
    "now_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "now_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = loaded_classifier.predict(df_copy)\n",
    "\n",
    "tmp = pd.read_csv('data/submission.csv')\n",
    "tmp.loc[:, 'LEVEL'] = y_pred\n",
    "tmp.to_csv('./sub/'+now_time+'_auto_sklearn.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_report(test_report):\n",
    "    lists = os.listdir(test_report)                                    #列出目錄的下所有文件和文件夾保存到lists\n",
    "    print(list)\n",
    "    lists.sort(key=lambda fn:os.path.getmtime(test_report + \"/\" + fn))#按時間排序\n",
    "    file_new = os.path.join(test_report,lists[-1])                     #獲取最新的文件保存到file_new\n",
    "    print(file_new)\n",
    "    return file_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "sub/2021-06-05 15:21:20_auto_sklearn.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    test_report=\"sub\"   #目錄地址\n",
    "    new_report(test_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "sub/2021-06-05 15:21:20_auto_sklearn.csv\n",
      "False\n",
      "115\n"
     ]
    }
   ],
   "source": [
    "a = pd.read_csv(new_report(test_report))['LEVEL']\n",
    "b = pd.read_csv('sub/auto_sklearn (1).csv')['LEVEL']\n",
    "\n",
    "\n",
    "print(np.array_equiv(a,b))\n",
    "tmp = np.where(a!=b)[0]\n",
    "print(len(tmp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "兩個答案不重複的這麼多? 實際上差異的東西看起來會不會是排名上差1或2? 是否表示跟排名上有些許差異。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 5 5 5 5 5 5 5 6 6 6 6 2 2 2 2 2 4 4 4 4 6 6 6 8 8 8 8 6 3 3 3 3 6 5 5 5\n",
      " 5 7 7 7 7 7 7 7 7 6 6 2 4 4 4 4 8 8 8 8 8 8 8 2 2 2 8 8 8 2 2 2 2 2 2 2 2\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 6 6 6 6 6 6 6 6 6 5 5 5 7 7 7 2 2 6 6 6 2 2\n",
      " 3 3 3 3]\n",
      "[ 2  7  7  7  7  7  7  7  4  2  1  2  1  1  7  7  1  7  1  2  2  7  7  2\n",
      "  6 10  6  6 10  6  6  6  6 10  6  6  6  6  2  2  2 10  6  6 10  6  2  2\n",
      " 10  2  2  2  2  9  9  9  2  2  2  2  9  8  8  9  2  2  8  8  8  8  7  1\n",
      "  1  1  2  2  2  2  5  7  7  7  5  5  1  1  3  3  3  5  5  5  7  7  5  7\n",
      "  5  3  3  3  3  2  9  2  5  6  9  5  2  1  1  2  2  7  2]\n"
     ]
    }
   ],
   "source": [
    "# test data\n",
    "print(a[np.where(a!=b)[0]].values)\n",
    "print(b[np.where(a!=b)[0]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 差太多了，思路不一樣，XGB那邊是用reg去側，然後四捨五入。autosklearn是直接分類\n",
    "# 005版"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
